"""Pipeline YAML template generation for typical DS pipeline."""


FRAMEWORK_CONFIGS = {
    "h2o_automl": {
        "yaml": """    framework: h2o_automl
    config:
      max_runtime_secs: 3600  # 1 hour
      max_models: 20
      seed: 42""",
    },
    "sklearn": {
        "yaml": """    framework: sklearn
    config:
      model: RandomForestClassifier
      n_estimators: 100
      random_state: 42""",
    },
    "xgboost": {
        "yaml": """    framework: xgboost
    config:
      n_estimators: 100
      max_depth: 6
      learning_rate: 0.1
      random_state: 42""",
    },
}


def generate_typical_pipeline_yaml(project_name: str, framework: str = "h2o_automl") -> str:
    """Generate example pipeline YAML for typical DS pattern.

    Args:
        project_name: Name of the project
        framework: ML framework (sklearn, h2o_automl, xgboost)

    Returns:
        YAML string for training pipeline
    """
    framework_config = FRAMEWORK_CONFIGS.get(framework, FRAMEWORK_CONFIGS["h2o_automl"])

    return f"""schema_version: 1

# =============================================================================
# Typical DS Pipeline: Churn Prediction with Multi-Table Features
# =============================================================================
#
# This pipeline demonstrates the typical data science workflow with:
#   - Multiple feature tables joined by [customer_id, snapshot_date]
#   - Temporal windowing (12 months train, 1 month test)
#   - High-dimensional features (2000+ features from multiple sources)
#   - Feature selection (variance, correlation, LGBM importance)
#   - Monthly retraining cadence with drift detection
#
# Generated by: mbt init --template typical-ds-pipeline
#
# =============================================================================

project:
  name: {project_name}_training_v1
  experiment_name: {project_name}_churn_prediction
  owner: data_science_team
  problem_type: binary_classification
  tags:
    - churn
    - typical-ds-pipeline
    - generated

deployment:
  mode: batch
  cadence: monthly

training:
  # ---------------------------------------------------------------------------
  # Data Source - Multi-table join pattern
  # ---------------------------------------------------------------------------
  data_source:
    label_table: label_table

    feature_tables:
      - table: features_table_a
        join_key: [customer_id, snapshot_date]
        join_type: left
        fan_out_check: true

      - table: features_table_b
        join_key: [customer_id, snapshot_date]
        join_type: left
        fan_out_check: true

    # Temporal windowing (relative mode)
    data_windows:
      logic: relative
      unit_type: months
      windows:
        test_lookback_units: 1
        train_gap_units: 0
        train_lookback_units: 12

  # ---------------------------------------------------------------------------
  # Schema
  # ---------------------------------------------------------------------------
  schema:
    target:
      label_column: is_churn
      classes: [0, 1]
      positive_class: 1

    identifiers:
      primary_key: customer_id
      partition_key: snapshot_date

    ignored_columns: []

  # ---------------------------------------------------------------------------
  # Feature Selection
  # ---------------------------------------------------------------------------
  feature_selection:
    enabled: true
    methods:
      - name: variance_threshold
        threshold: 0.0
      - name: correlation
        threshold: 0.9
      - name: lgbm_importance
        threshold: 0.95

  # ---------------------------------------------------------------------------
  # Model Training
  # ---------------------------------------------------------------------------
  model_training:
{framework_config["yaml"]}

  # ---------------------------------------------------------------------------
  # Evaluation
  # ---------------------------------------------------------------------------
  evaluation:
    primary_metric: roc_auc
    additional_metrics:
      - accuracy
      - f1
      - precision
      - recall

    generate_plots: true

    # Monthly performance and drift analysis
    temporal_analysis:
      enabled: true
      partition_key: snapshot_date
      metrics:
        - roc_auc
        - psi
"""


def generate_typical_pipeline_yaml_with_feature_selection(project_name: str, framework: str = "h2o_automl") -> str:
    """Generate pipeline YAML with advanced feature selection.

    Args:
        project_name: Name of the project
        framework: ML framework (sklearn, h2o_automl, xgboost)

    Returns:
        YAML string for training pipeline with feature selection
    """
    framework_config = FRAMEWORK_CONFIGS.get(framework, FRAMEWORK_CONFIGS["h2o_automl"])

    return f"""schema_version: 1

# =============================================================================
# Typical DS Pipeline with Advanced Feature Selection
# =============================================================================

project:
  name: {project_name}_training_advanced_v1
  experiment_name: {project_name}_churn_prediction
  owner: data_science_team
  problem_type: binary_classification
  tags:
    - churn
    - typical-ds-pipeline
    - feature-selection

deployment:
  mode: batch
  cadence: monthly

training:
  data_source:
    label_table: label_table

    feature_tables:
      - table: features_table_a
        join_key: [customer_id, snapshot_date]
        join_type: left
        fan_out_check: true

      - table: features_table_b
        join_key: [customer_id, snapshot_date]
        join_type: left
        fan_out_check: true

    data_windows:
      logic: relative
      unit_type: months
      windows:
        test_lookback_units: 1
        train_gap_units: 0
        train_lookback_units: 12

  schema:
    target:
      label_column: is_churn
      classes: [0, 1]
      positive_class: 1

    identifiers:
      primary_key: customer_id
      partition_key: snapshot_date

    ignored_columns: []

  # ---------------------------------------------------------------------------
  # Advanced Feature Selection Pipeline
  # ---------------------------------------------------------------------------
  feature_selection:
    enabled: true
    methods:
      - name: variance_threshold
        threshold: 0.0
      - name: correlation
        threshold: 0.9
      - name: lgbm_importance
        threshold: 0.95

  model_training:
{framework_config["yaml"]}

  evaluation:
    primary_metric: roc_auc
    additional_metrics: [accuracy, f1, precision, recall]
    generate_plots: true

    temporal_analysis:
      enabled: true
      partition_key: snapshot_date
      metrics: [roc_auc, psi]
"""


def generate_profiles_yaml(project_name: str) -> str:
    """Generate profiles.yaml for typical DS pipeline.

    Args:
        project_name: Name of the project

    Returns:
        YAML string for profiles configuration
    """
    return f"""{project_name}:
  target: dev

  mlflow:
    experiment_name: {project_name}_experiment

  outputs:
    # =========================================================================
    # Development environment (local execution)
    # =========================================================================
    dev:
      executor:
        type: local

      storage:
        type: local
        config:
          base_path: ./local_artifacts

      data_connector:
        type: local_file
        config:
          data_path: ./sample_data

      mlflow:
        tracking_uri: "./mlruns"

      secrets:
        provider: env

    # =========================================================================
    # Production environment (example - update with your infrastructure)
    # =========================================================================
    prod:
      executor:
        type: kubernetes
        config:
          namespace: ml-production
          image: your-registry/mbt-runner:latest

      storage:
        type: s3
        config:
          bucket: ml-artifacts-prod
          prefix: {project_name}/

      data_connector:
        type: snowflake  # or postgres, bigquery, duckdb, etc.
        config:
          account: your-account
          warehouse: COMPUTE_WH
          database: ANALYTICS
          schema: ML_FEATURES

      mlflow:
        tracking_uri: "https://mlflow.yourcompany.com"

      secrets:
        provider: aws_secrets_manager
"""


def generate_readme(project_name: str, config: dict) -> str:
    """Generate comprehensive README for the generated project.

    Args:
        project_name: Name of the project
        config: Data generation configuration dict

    Returns:
        Markdown string for README.md
    """
    num_customers = config.get('num_customers', 10000)
    num_features_a = config.get('num_features_a', 1000)
    num_features_b = config.get('num_features_b', 1000)
    start_date = config.get('start_date', '2025-01-01')
    end_date = config.get('end_date', 'today')
    daily_samples = config.get('daily_samples', 10000)

    return f"""# {project_name}

MBT project demonstrating typical data science pipeline pattern for churn prediction.

## Dataset

This project contains synthetic churn prediction data generated with realistic characteristics:

- **{num_customers:,} unique customers**
- **{num_features_a:,} features in table A**
- **{num_features_b:,} features in table B**
- **Date range:** {start_date} to {end_date}
- **Daily samples:** {daily_samples:,} per day

### Tables

1. **`label_table.csv`** - Customer churn labels
   - Columns: `customer_id`, `snapshot_date`, `is_churn`
   - Limited by 3-month label lag (recent months unlabeled)

2. **`features_table_a.csv`** - Feature set A
   - Columns: `customer_id`, `snapshot_date`, `features_a_0001` ... `features_a_{num_features_a:04d}`
   - Realistic data quality issues (missing values, constant features, correlations)

3. **`features_table_b.csv`** - Feature set B
   - Columns: `customer_id`, `snapshot_date`, `features_b_0001` ... `features_b_{num_features_b:04d}`
   - Similar data quality characteristics as table A

4. **`customers_to_score.csv`** - Monthly customer snapshots for batch scoring
   - Columns: `customer_id`, `snapshot_date`
   - Used for serving/scoring pipeline

### Data Quality Characteristics

The synthetic data includes realistic production issues:
- **Missing values:** ~15% across features
- **Constant features:** ~5% of features have no variance
- **High correlation:** ~50 pairs of highly correlated features (r > 0.9)
- **Temporal patterns:** Customer churn probability varies by customer segment

## Pipeline Architecture

### Training Pipeline: `{project_name}_training_v1`

The main training pipeline demonstrates production ML workflows:

1. **Multi-table data loading**
   - Loads label table + 2 feature tables
   - Joins on `[customer_id, snapshot_date]`
   - Fan-out detection prevents data duplication bugs

2. **Temporal windowing**
   - Relative mode: windows shift with execution date
   - Train: 12 months historical data
   - Test: 1 month most recent data
   - Gap: 0 months (configurable)

3. **Feature preprocessing** (optional, in advanced pipeline)
   - Removes constant features
   - Removes highly correlated features
   - LGBM-based importance selection

4. **Model training**
   - H2O AutoML with 1-hour runtime limit
   - Trains up to 20 models
   - Automatic algorithm selection and hyperparameter tuning

5. **Evaluation**
   - Primary metric: ROC-AUC
   - Additional metrics: accuracy, F1, precision, recall
   - Monthly drift detection (PSI) for distribution shifts

## Getting Started

### Prerequisites

```bash
# Install MBT core
pip install mbt-core

# Install H2O AutoML adapter (optional)
pip install mbt-h2o

# Install LightGBM for feature selection (optional)
pip install lightgbm
```

### Run Training Pipeline

```bash
# Compile the pipeline
mbt compile {project_name}_training_v1

# Run training locally
mbt run --select {project_name}_training_v1

# Run with specific execution date
mbt run --select {project_name}_training_v1 --vars execution_date=2026-01-15

# Check results
ls -lh local_artifacts/run_*/
cat local_artifacts/run_*/metrics.json
```

### View Pipeline DAG

```bash
# Show pipeline structure
mbt dag {project_name}_training_v1
```

### Run with Different Profile

```bash
# Use production profile (requires infrastructure setup)
mbt run --select {project_name}_training_v1 --profile prod
```

## Configuration Files

- **`pipelines/{project_name}_training_v1.yaml`** - Main training pipeline
- **`profiles.yaml`** - Environment-specific configuration (dev, prod)
- **`sample_data/`** - Generated synthetic data (CSV files)

## Customization

### Modify Data Windows

Edit `pipelines/{project_name}_training_v1.yaml`:

```yaml
data_windows:
  logic: relative  # or 'absolute' for fixed dates
  unit_type: months  # or days, weeks, quarters, years
  windows:
    test_lookback_units: 1  # Test set size
    train_gap_units: 0  # Gap between train and test
    train_lookback_units: 12  # Training history
```

### Add Feature Selection

Use the advanced pipeline or add to main pipeline:

```yaml
feature_selection:
  enabled: true
  methods:
    - name: variance_threshold
      threshold: 0.0
    - name: correlation
      threshold: 0.9
    - name: lgbm_importance
      threshold: 0.95
```

### Connect to Real Data Sources

Update `profiles.yaml` to connect to your data warehouse:

```yaml
data_connector:
  type: snowflake  # or postgres, bigquery, etc.
  config:
    account: your-account
    warehouse: COMPUTE_WH
    database: ANALYTICS
    schema: ML_FEATURES
```

## Advanced Features

### Drift Detection

The pipeline automatically calculates Population Stability Index (PSI) for each feature across months:

```bash
# View drift analysis results
cat local_artifacts/run_*/drift_info.csv
```

PSI interpretation:
- PSI < 0.1: No significant change
- 0.1 ≤ PSI < 0.2: Moderate drift
- PSI ≥ 0.2: Significant drift (investigate further)

### Monthly Retraining

Configure for production monthly retraining:

```yaml
deployment:
  mode: batch
  cadence: monthly
```

Generate Airflow DAG:

```bash
mbt compile {project_name}_training_v1 --orchestrator airflow
```

## Project Structure

```
{project_name}/
├── pipelines/
│   └── {project_name}_training_v1.yaml    # Training pipeline definition
├── sample_data/
│   ├── label_table.csv                    # Churn labels
│   ├── features_table_a.csv               # Feature set A
│   ├── features_table_b.csv               # Feature set B
│   └── customers_to_score.csv             # Scoring population
├── local_artifacts/                        # Training outputs (gitignored)
├── mlruns/                                 # MLflow tracking (gitignored)
├── lib/                                    # Custom transforms (optional)
├── tests/                                  # Unit tests (optional)
├── profiles.yaml                           # Environment configuration
├── pyproject.toml                          # Python package config
└── README.md                               # This file
```

## Production Deployment

1. **Update profiles.yaml** with production infrastructure
2. **Configure data connector** to point to production data warehouse
3. **Set up MLflow tracking** with production tracking URI
4. **Generate orchestrator DAG** (Airflow, Prefect, or Dagster)
5. **Deploy to Kubernetes** or other execution environment
6. **Schedule monthly runs** via orchestrator

## Troubleshooting

### Out of Memory Error

If you encounter memory issues with large feature sets:
- Reduce `num_features_a` or `num_features_b` in regeneration
- Enable feature selection to reduce dimensionality
- Use chunked data loading (contact support)

### Temporal Split Returns Empty Data

Ensure execution_date and data_windows are configured correctly:
- Check available data date range in CSV files
- Verify execution_date is within available range
- Account for label lag (3 months for churn)

### MLflow Tracking Errors

Ensure MLflow server is running and accessible:
```bash
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns
```

## Contributing

This is a generated example project. Customize as needed for your use case!

## License

MIT License - feel free to use and modify.

---

**Generated by:** MBT (Model Build Tool)
**Template:** typical-ds-pipeline
**Generated on:** {start_date}
"""
