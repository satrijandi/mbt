"""
Auto-generated by MBT. Do not edit.
Pipeline: training_churn_model_v1
Target: docker
"""

from airflow.sdk import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from docker.types import Mount
from datetime import datetime, timedelta

default_args = {
    "owner": "de-team",
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

_project_mount = Mount(
    target="/opt/mbt/project",
    source="/home/satrijandi/code/mbt/integration-test/project",
    type="bind",
    read_only=False,
)

with DAG(
    dag_id="training_churn_model_v1",
    default_args=default_args,
    schedule="@daily",
    start_date=datetime(2026, 1, 1),
    catchup=False,
    tags=["mbt", "training_churn_model_v1"],
) as dag:

    load_data = DockerOperator(
        task_id="load_data",
        image="mbt-runner:latest",
        command=(
            "step execute"
            " --pipeline training_churn_model_v1"
            " --step load_data"
            " --target docker"
            ' --run-id "run_{{ ds_nodash }}_{{ ts_nodash }}"'
        ),
        working_dir="/opt/mbt/project",
        mounts=[_project_mount],
        network_mode="integration-test_default",
        environment={
            "AWS_ACCESS_KEY_ID": "any",
            "AWS_SECRET_ACCESS_KEY": "any",
            "MLFLOW_S3_ENDPOINT_URL": "http://seaweedfs-filer:8333",
        },
        docker_url="unix://var/run/docker.sock",
        auto_remove="success",
        mount_tmp_dir=False,
    )

    split_data = DockerOperator(
        task_id="split_data",
        image="mbt-runner:latest",
        command=(
            "step execute"
            " --pipeline training_churn_model_v1"
            " --step split_data"
            " --target docker"
            ' --run-id "run_{{ ds_nodash }}_{{ ts_nodash }}"'
        ),
        working_dir="/opt/mbt/project",
        mounts=[_project_mount],
        network_mode="integration-test_default",
        environment={
            "AWS_ACCESS_KEY_ID": "any",
            "AWS_SECRET_ACCESS_KEY": "any",
            "MLFLOW_S3_ENDPOINT_URL": "http://seaweedfs-filer:8333",
        },
        docker_url="unix://var/run/docker.sock",
        auto_remove="success",
        mount_tmp_dir=False,
    )

    train_model = DockerOperator(
        task_id="train_model",
        image="mbt-runner:latest",
        command=(
            "step execute"
            " --pipeline training_churn_model_v1"
            " --step train_model"
            " --target docker"
            ' --run-id "run_{{ ds_nodash }}_{{ ts_nodash }}"'
        ),
        working_dir="/opt/mbt/project",
        mounts=[_project_mount],
        network_mode="integration-test_default",
        environment={
            "AWS_ACCESS_KEY_ID": "any",
            "AWS_SECRET_ACCESS_KEY": "any",
            "MLFLOW_S3_ENDPOINT_URL": "http://seaweedfs-filer:8333",
        },
        docker_url="unix://var/run/docker.sock",
        auto_remove="success",
        mount_tmp_dir=False,
    )

    evaluate = DockerOperator(
        task_id="evaluate",
        image="mbt-runner:latest",
        command=(
            "step execute"
            " --pipeline training_churn_model_v1"
            " --step evaluate"
            " --target docker"
            ' --run-id "run_{{ ds_nodash }}_{{ ts_nodash }}"'
        ),
        working_dir="/opt/mbt/project",
        mounts=[_project_mount],
        network_mode="integration-test_default",
        environment={
            "AWS_ACCESS_KEY_ID": "any",
            "AWS_SECRET_ACCESS_KEY": "any",
            "MLFLOW_S3_ENDPOINT_URL": "http://seaweedfs-filer:8333",
        },
        docker_url="unix://var/run/docker.sock",
        auto_remove="success",
        mount_tmp_dir=False,
    )

    log_run = DockerOperator(
        task_id="log_run",
        image="mbt-runner:latest",
        command=(
            "step execute"
            " --pipeline training_churn_model_v1"
            " --step log_run"
            " --target docker"
            ' --run-id "run_{{ ds_nodash }}_{{ ts_nodash }}"'
        ),
        working_dir="/opt/mbt/project",
        mounts=[_project_mount],
        network_mode="integration-test_default",
        environment={
            "AWS_ACCESS_KEY_ID": "any",
            "AWS_SECRET_ACCESS_KEY": "any",
            "MLFLOW_S3_ENDPOINT_URL": "http://seaweedfs-filer:8333",
        },
        docker_url="unix://var/run/docker.sock",
        auto_remove="success",
        mount_tmp_dir=False,
    )

    # Dependencies
    load_data >> split_data
    split_data >> train_model
    train_model >> evaluate
    evaluate >> log_run
